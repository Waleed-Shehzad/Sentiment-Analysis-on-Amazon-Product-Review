# -*- coding: utf-8 -*-
"""Task 1: Sentiment Analysis on Product Reviews .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12_okR8vyRLf0tKGgRfkD74bN5DBGQfk-
"""

# ===============================
# Install dependencies
# ===============================
!pip install kagglehub -q
!pip install -U spacy -q
!python -m spacy download en_core_web_sm -q

import kagglehub
import os
import pandas as pd
import re
import spacy
from tqdm import tqdm
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score


# Step 1: Download dataset

path = kagglehub.dataset_download("arhamrumi/amazon-product-reviews")
print("Dataset downloaded to:", path)

# Detect CSV file
csv_files = [f for f in os.listdir(path) if f.endswith(".csv")]
if not csv_files:
    raise FileNotFoundError("No CSV files found in dataset folder!")
csv_file = csv_files[0]
df = pd.read_csv(os.path.join(path, csv_file))
print(f"Loaded file: {csv_file}")
print(df.head())


# Step 2: Map ratings to sentiment

if 'Score' in df.columns:
    df['sentiment'] = df['Score'].apply(lambda x: 1 if x >= 4 else 0)
else:
    raise ValueError("Dataset does not contain a 'Score' column.")


# Step 3: spaCy preprocessing

nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])
stop_words = set(stopwords.words('english'))

def clean_review(text):
    if pd.isna(text):
        return ""
    text = re.sub(r'<.*?>', '', text)        # Remove HTML tags
    text = re.sub(r'[^a-zA-Z]', ' ', text)   # Keep only letters
    doc = nlp(text.lower())
    tokens = [token.lemma_ for token in doc
              if token.is_alpha and not token.is_stop and len(token) > 2]
    return ' '.join(tokens)

tqdm.pandas(desc="Cleaning")
df['cleaned_review'] = df['Text'].progress_apply(clean_review)


# Step 4: TF-IDF + Logistic Regression

tfidf = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf.fit_transform(df['cleaned_review'])
y = df['sentiment'].astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf, y, test_size=0.2, random_state=42, stratify=y
)

lr_model = LogisticRegression(max_iter=200)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

print("\n=== Logistic Regression ===")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

# Confusion Matrix
cm_lr = confusion_matrix(y_test, y_pred_lr)
sns.heatmap(cm_lr, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title("Logistic Regression Confusion Matrix")
plt.show()


# Step 5: Visualize frequent words

positive_reviews = df[df['sentiment'] == 1]['cleaned_review']
negative_reviews = df[df['sentiment'] == 0]['cleaned_review']

def plot_top_words(texts, title, color):
    all_words = ' '.join(texts).split()
    common = Counter(all_words).most_common(20)
    word_df = pd.DataFrame(common, columns=['word', 'count'])
    plt.figure(figsize=(10,5))
    sns.barplot(data=word_df, x='count', y='word', palette=color)
    plt.title(title)
    plt.show()

plot_top_words(positive_reviews, "Top 20 Positive Words", 'Greens_r')
plot_top_words(negative_reviews, "Top 20 Negative Words", 'Reds_r')

# Step 6: Naive Bayes with CountVectorizer

cv = CountVectorizer()
X_cv = cv.fit_transform(df['cleaned_review'])
X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(
    X_cv, y, test_size=0.2, random_state=42, stratify=y
)

nb_model = MultinomialNB()
nb_model.fit(X_train_cv, y_train_cv)
y_pred_nb = nb_model.predict(X_test_cv)

print("\n=== Naive Bayes ===")
print("Accuracy:", accuracy_score(y_test_cv, y_pred_nb))
print(classification_report(y_test_cv, y_pred_nb))

cm_nb = confusion_matrix(y_test_cv, y_pred_nb)
sns.heatmap(cm_nb, annot=True, fmt="d", cmap="Oranges",
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title("Naive Bayes Confusion Matrix")
plt.show()